{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import pydotplus\n",
    "\n",
    "# Pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from IPython.display import Image  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "from imblearn.under_sampling import NearMiss, InstanceHardnessThreshold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "from imblearn.combine import SMOTEENN \n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Metrics, model evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, cross_val_predict\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_results_tr_ts(model, X_train, y_train, X_test, y_test, name):\n",
    "    \"\"\"\n",
    "    Function to evaluate one or multiple models and check train and test results with parameters set.\n",
    "    \n",
    "    Model fitting is done inside of the function.\n",
    "    \n",
    "    The function takes model, split data, and model name and returns the results on test and train sets. \n",
    "    Test set: confusion matrix, classification report. \n",
    "    Train and test sets: dataframe with accuracy, precision, recall, F1 score, and rocauc \n",
    "    score done with cross-validation.\n",
    "    \"\"\"\n",
    "    print(f'**{name}** results', '\\n')\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test  = model.predict(X_test)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "    \n",
    "    # Visualizing confusion matrix\n",
    "    print(f'Confusion Matrix of test data')\n",
    "    group_names  = ['TN', 'FP', 'FN', 'TP']\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n",
    "    labels       = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names, group_counts)]\n",
    "    labels       = np.asarray(labels).reshape(2,2)\n",
    "    fig, ax      = plt.subplots(figsize=(8,5))\n",
    "    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='BuGn', ax=ax)\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(f'Classifictation Report of test data', '\\n')\n",
    "    print(classification_report(y_test, y_pred_test),'\\n')\n",
    "    \n",
    "    # Main metrics with cross_val_score\n",
    "    print(f'Main model evaluation metrics with cross validation (train and test data)','\\n')\n",
    "    strat_k_fold   = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracy       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='accuracy'))\n",
    "    precision      = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='precision'))\n",
    "    recall         = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='recall'))\n",
    "    f1score        = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='f1'))\n",
    "    rocauc         = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='roc_auc'))\n",
    "    \n",
    "    df_model_train = pd.DataFrame({'data'         : 'train',\n",
    "                                   'model'        : [name],\n",
    "                                   'accuracy'     : [accuracy],\n",
    "                                   'precision'    : [precision],\n",
    "                                   'recall'       : [recall],\n",
    "                                   'f1score'      : [f1score],\n",
    "                                   'rocauc'       : [rocauc]})\n",
    "   \n",
    "    accuracy      = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='accuracy'))\n",
    "    precision     = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='precision'))\n",
    "    recall        = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='recall'))\n",
    "    f1score       = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='f1'))\n",
    "    rocauc        = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='roc_auc'))\n",
    "    \n",
    "    df_model_test = pd.DataFrame({'data'         : 'test',\n",
    "                                  'model'        : [name],\n",
    "                                  'accuracy'     : [accuracy],\n",
    "                                  'precision'    : [precision],\n",
    "                                  'recall'       : [recall],\n",
    "                                  'f1score'      : [f1score],\n",
    "                                  'rocauc'       : [rocauc]})\n",
    "    \n",
    "    df_model = pd.concat([df_model_train, df_model_test], ignore_index=True)\n",
    "    \n",
    "    print(df_model.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_results_test_df(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Function to evaluate multiple models on test data with parameters set.\n",
    "    \n",
    "    Model fitting is done inside of the function.\n",
    "    \n",
    "    The function takes model, split data and the name of the model and returns the results of the test \n",
    "    data: dataframe with accuracy, precision, recall, F1 score and rocauc score done with \n",
    "    cross-validation.\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Main metrics with cross_val_score\n",
    "    strat_k_fold   = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracy       = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='accuracy'))\n",
    "    precision      = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='precision'))\n",
    "    recall         = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='recall'))\n",
    "    f1score        = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='f1'))\n",
    "    rocauc         = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='roc_auc'))\n",
    "    \n",
    "    df_model_test  = pd.DataFrame({'data'         : 'test',\n",
    "                                   'model'        : [model_name],\n",
    "                                   'accuracy'     : [accuracy],\n",
    "                                   'precision'    : [precision],\n",
    "                                   'recall'       : [recall],\n",
    "                                   'f1score'      : [f1score],\n",
    "                                   'rocauc'       : [rocauc]})\n",
    "    \n",
    "    return df_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_results_test_cm_cr_df(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Function to evaluate multiple models on test data with parameters set.\n",
    "    \n",
    "    Model fitting is done inside of the function.\n",
    "    \n",
    "    The function takes model, test data (X and y) and model name and returns the results on test set:\n",
    "    confusion matrix, classification report and dataframe with accuracy, precision, recall, F1 score\n",
    "    and rocauc score done with cross-validation.\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    print('\\n', f'**{model_name}** results', '\\n')\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "    \n",
    "    # Visualizing confusion matrix\n",
    "    print(f'Confusion Matrix of test data')\n",
    "    group_names  = ['TN', 'FP', 'FN', 'TP']\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n",
    "    labels       = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names, group_counts)]\n",
    "    labels       = np.asarray(labels).reshape(2,2)\n",
    "    fig, ax      = plt.subplots(figsize=(8,5))\n",
    "    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='BuGn', ax=ax)\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(f'Classifictation Report of test data', '\\n')\n",
    "    print(classification_report(y_test, y_pred_test),'\\n')\n",
    "    \n",
    "    # Main metrics with cross_val_score\n",
    "    strat_k_fold   = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracy       = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='accuracy'))\n",
    "    precision      = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='precision'))\n",
    "    recall         = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='recall'))\n",
    "    f1score        = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='f1'))\n",
    "    rocauc         = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='roc_auc'))\n",
    "    \n",
    "    df_model_test  = pd.DataFrame({'data'         : 'test',\n",
    "                                   'model'        : [model_name],\n",
    "                                   'accuracy'     : [accuracy],\n",
    "                                   'precision'    : [precision],\n",
    "                                   'recall'       : [recall],\n",
    "                                   'f1score'      : [f1score],\n",
    "                                   'rocauc'       : [rocauc]})\n",
    "    \n",
    "    return df_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_results_test_cm_df(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Function to evaluate multiple models on test data with parameters set.\n",
    "    \n",
    "    Model fitting is done inside of the function.\n",
    "    \n",
    "    The function takes the model, split data and the model name and returns the results on the \n",
    "    test set:\n",
    "    confusion matrix and the dataframe with accuracy, precision, recall, F1 score\n",
    "    and rocauc score done with cross-validation.\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    print('\\n', f'**{model_name}** results', '\\n')\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "    \n",
    "    # Visualizing confusion matrix\n",
    "    print(f'Confusion Matrix of test data')\n",
    "    group_names  = ['TN', 'FP', 'FN', 'TP']\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n",
    "    labels       = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names, group_counts)]\n",
    "    labels       = np.asarray(labels).reshape(2,2)\n",
    "    fig, ax      = plt.subplots(figsize=(8,5))\n",
    "    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='BuGn', ax=ax)\n",
    "    plt.show()\n",
    "    \n",
    "    # Main metrics with cross_val_score\n",
    "    strat_k_fold   = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracy       = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='accuracy'))\n",
    "    precision      = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='precision'))\n",
    "    recall         = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='recall'))\n",
    "    f1score        = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='f1'))\n",
    "    rocauc         = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='roc_auc'))\n",
    "    \n",
    "    df_model_test  = pd.DataFrame({'data'         : 'test',\n",
    "                                   'model'        : [model_name],\n",
    "                                   'accuracy'     : [accuracy],\n",
    "                                   'precision'    : [precision],\n",
    "                                   'recall'       : [recall],\n",
    "                                   'f1score'      : [f1score],\n",
    "                                   'rocauc'       : [rocauc]})\n",
    "    \n",
    "    return df_model_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_results_hyp_test_cm_cr_df(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Function to evaluate one or multiple models on test set with hyperparameter tuning.\n",
    "    \n",
    "    Model fitting is done outside of the function (when doing grid search).\n",
    "    \n",
    "    The function takes model, test data (X and y) and model name and returns the results on test set:\n",
    "    confusion matrix, classification report and dataframe with accuracy, precision, recall, F1 score\n",
    "    and rocauc score done with cross-validation.\n",
    "    \"\"\"\n",
    "    print('\\n', f'**{model_name}** results', '\\n')\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "    \n",
    "    # Visualizing confusion matrix\n",
    "    print(f'Confusion Matrix of test data')\n",
    "    group_names  = ['TN', 'FP', 'FN', 'TP']\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n",
    "    labels       = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names, group_counts)]\n",
    "    labels       = np.asarray(labels).reshape(2,2)\n",
    "    fig, ax      = plt.subplots(figsize=(8,5))\n",
    "    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='BuGn', ax=ax)\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(f'Classifictation Report of test data', '\\n')\n",
    "    print(classification_report(y_test, y_pred_test),'\\n')\n",
    "    \n",
    "    # Main metrics with cross_val_score\n",
    "    strat_k_fold   = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    accuracy       = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='accuracy'))\n",
    "    precision      = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='precision'))\n",
    "    recall         = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='recall'))\n",
    "    f1score        = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='f1'))\n",
    "    rocauc         = np.mean(cross_val_score(model, X_test, y_test, cv=strat_k_fold, scoring='roc_auc'))\n",
    "    \n",
    "    df_model_test  = pd.DataFrame({'data'         : 'test',\n",
    "                                   'model'        : [model_name],\n",
    "                                   'accuracy'     : [accuracy],\n",
    "                                   'precision'    : [precision],\n",
    "                                   'recall'       : [recall],\n",
    "                                   'f1score'      : [f1score],\n",
    "                                   'rocauc'       : [rocauc]})\n",
    "    \n",
    "    return df_model_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Introduction\n",
    "After exploring and preprocessing our data we are ready to start working with Machine Learning algorithms. Before diving into models, let's define what problem we have, what models should be used, and which metrics will be used for evaluation.\n",
    "\n",
    "We have a binary classification problem, we want to predict if a customer is going to churn after the first month or not (two classes).\n",
    "We will be using supervised machine learning models. We will train our model with historical data that has an outcome specified.\n",
    "The best metric in our case is F1 score, which shows the balance between precision and recall. We will also be using the confusion matrix to see how many customers have fallen under correct categories (True Positive and True Negative) and how many were mistaken (False Positive and False Negative). \n",
    "In the case of improving precision or recall, we will be focusing on improving recall. High precision relates to a low False Positive rate, and high recall relates to a low False Negative rate. The lower False Negative rate, the better. In our case, False Negative entries indicate that the customers will not churn but actually they will. Thinking that customer is not going to churn, we are losing opportunities because we are not putting appropriate attention to his/hers case. It means that to recover this loss, we would need to invest more, to again acquire a new customer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventTime_ay</th>\n",
       "      <th>Date</th>\n",
       "      <th>email</th>\n",
       "      <th>firstName</th>\n",
       "      <th>lastName</th>\n",
       "      <th>Sub_Categoria</th>\n",
       "      <th>campaign</th>\n",
       "      <th>medium</th>\n",
       "      <th>userId</th>\n",
       "      <th>singular_duplicate</th>\n",
       "      <th>test_duplicate</th>\n",
       "      <th>row_numbers</th>\n",
       "      <th>extCode</th>\n",
       "      <th>Comercio</th>\n",
       "      <th>RC</th>\n",
       "      <th>session_count_email</th>\n",
       "      <th>session_count_userId</th>\n",
       "      <th>eventTime_ac</th>\n",
       "      <th>gclid</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>town</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>policy_start_date</th>\n",
       "      <th>policy_end_date</th>\n",
       "      <th>floors</th>\n",
       "      <th>goods</th>\n",
       "      <th>surface</th>\n",
       "      <th>inauguration_year</th>\n",
       "      <th>locationRisk</th>\n",
       "      <th>numLocals</th>\n",
       "      <th>shop_postcalcode</th>\n",
       "      <th>refurbishing_year</th>\n",
       "      <th>shop_region</th>\n",
       "      <th>electronic_protections</th>\n",
       "      <th>fire_protections</th>\n",
       "      <th>theft_protections</th>\n",
       "      <th>watchman_protections</th>\n",
       "      <th>Type</th>\n",
       "      <th>eventTime_vg</th>\n",
       "      <th>gross_income</th>\n",
       "      <th>money_invested</th>\n",
       "      <th>furnitureWithEquipment</th>\n",
       "      <th>merchandise</th>\n",
       "      <th>ownership</th>\n",
       "      <th>eventTime_off</th>\n",
       "      <th>monthlypayment1</th>\n",
       "      <th>premiumPrice1</th>\n",
       "      <th>monthlypayment2</th>\n",
       "      <th>premiumPrice2</th>\n",
       "      <th>monthlypayment3</th>\n",
       "      <th>premiumPrice3</th>\n",
       "      <th>provider</th>\n",
       "      <th>eventTime_fd</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>documenttype</th>\n",
       "      <th>offerchosen</th>\n",
       "      <th>billingcycle</th>\n",
       "      <th>eventTime_pm</th>\n",
       "      <th>subcription_id</th>\n",
       "      <th>intent_status</th>\n",
       "      <th>endDate</th>\n",
       "      <th>paymenttype</th>\n",
       "      <th>policynumber</th>\n",
       "      <th>policy_url</th>\n",
       "      <th>policyprice</th>\n",
       "      <th>currentprice</th>\n",
       "      <th>Sales Date</th>\n",
       "      <th>Sub-categoria</th>\n",
       "      <th>Billing Cycle</th>\n",
       "      <th>Policy Number</th>\n",
       "      <th>Policy URL</th>\n",
       "      <th>Current Price</th>\n",
       "      <th>Policy Price</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>Payment Type</th>\n",
       "      <th>Persona</th>\n",
       "      <th>Fecha Anulación</th>\n",
       "      <th>Razon anulación</th>\n",
       "      <th>Aseguradora</th>\n",
       "      <th>Online/offline</th>\n",
       "      <th>Renovado</th>\n",
       "      <th>Precio Renovacion</th>\n",
       "      <th>Tipo de póliza</th>\n",
       "      <th>Unnamed: 86</th>\n",
       "      <th>Match email</th>\n",
       "      <th>Match telefono</th>\n",
       "      <th>Semana</th>\n",
       "      <th>Mes</th>\n",
       "      <th>WP</th>\n",
       "      <th>Dia Lead</th>\n",
       "      <th>quote</th>\n",
       "      <th>sale</th>\n",
       "      <th>Categoria</th>\n",
       "      <th>time_diff_ay_ac</th>\n",
       "      <th>time_diff_ac_vg</th>\n",
       "      <th>time_diff_vg_ff</th>\n",
       "      <th>time_diff_ff_fd</th>\n",
       "      <th>time_diff_fd_pm</th>\n",
       "      <th>city</th>\n",
       "      <th>cityId</th>\n",
       "      <th>regionId</th>\n",
       "      <th>sessions</th>\n",
       "      <th>alimentacion-y-dietetica</th>\n",
       "      <th>arte-y-entretenimiento</th>\n",
       "      <th>automocion</th>\n",
       "      <th>calzado-marroqueria-y-piel</th>\n",
       "      <th>confecciones-y-textiles</th>\n",
       "      <th>construccion-y-reformas</th>\n",
       "      <th>deportes-y-ocio</th>\n",
       "      <th>educacion</th>\n",
       "      <th>electronica-y-maquinaria</th>\n",
       "      <th>fabricacion</th>\n",
       "      <th>hosteleria-y-restauracion</th>\n",
       "      <th>muebles-decoracion-y-menaje-hogar</th>\n",
       "      <th>negocio-inmobiliario</th>\n",
       "      <th>peluquerias-y-centros-estetica</th>\n",
       "      <th>profesionales-economia</th>\n",
       "      <th>profesionales-oficina</th>\n",
       "      <th>rc-tecnicas-y-profesionales</th>\n",
       "      <th>servicios-generales</th>\n",
       "      <th>servicios-sanitarios</th>\n",
       "      <th>transporte-y-almacenamiento</th>\n",
       "      <th>varios</th>\n",
       "      <th>(not set)</th>\n",
       "      <th>Andalusia</th>\n",
       "      <th>Andorra la Vella</th>\n",
       "      <th>Aragon</th>\n",
       "      <th>Asturias</th>\n",
       "      <th>Azuay</th>\n",
       "      <th>Balearic Islands</th>\n",
       "      <th>Basque Country</th>\n",
       "      <th>Bavaria</th>\n",
       "      <th>Bogota</th>\n",
       "      <th>Buenos Aires</th>\n",
       "      <th>Canary Islands</th>\n",
       "      <th>Cantabria</th>\n",
       "      <th>Capital District</th>\n",
       "      <th>Cartago Province</th>\n",
       "      <th>Castile and Leon</th>\n",
       "      <th>Castile-La Mancha</th>\n",
       "      <th>Catalonia</th>\n",
       "      <th>Ceuta</th>\n",
       "      <th>Community of Madrid</th>\n",
       "      <th>England</th>\n",
       "      <th>Extremadura</th>\n",
       "      <th>Galicia</th>\n",
       "      <th>Georgia</th>\n",
       "      <th>Hessen</th>\n",
       "      <th>Ile-de-France</th>\n",
       "      <th>La Rioja</th>\n",
       "      <th>Lombardy</th>\n",
       "      <th>Madrid</th>\n",
       "      <th>Melilla</th>\n",
       "      <th>Morelos</th>\n",
       "      <th>Murcia</th>\n",
       "      <th>Navarre</th>\n",
       "      <th>New Hampshire</th>\n",
       "      <th>New York</th>\n",
       "      <th>North Holland</th>\n",
       "      <th>North Rhine-Westphalia</th>\n",
       "      <th>Occitanie</th>\n",
       "      <th>Pays de la Loire</th>\n",
       "      <th>Porto District</th>\n",
       "      <th>Prague</th>\n",
       "      <th>Region of Murcia</th>\n",
       "      <th>Rio Grande</th>\n",
       "      <th>Santa Fe Province</th>\n",
       "      <th>Santiago Metropolitan Region</th>\n",
       "      <th>Scotland</th>\n",
       "      <th>Utrecht</th>\n",
       "      <th>Valencian Community</th>\n",
       "      <th>Vienna</th>\n",
       "      <th>Zulia</th>\n",
       "      <th>GoogleDisplay</th>\n",
       "      <th>SingularC.email.billage</th>\n",
       "      <th>adwords</th>\n",
       "      <th>bing</th>\n",
       "      <th>facebook</th>\n",
       "      <th>hs_automation</th>\n",
       "      <th>hubspot</th>\n",
       "      <th>trustpilot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [eventTime_ay, Date, email, firstName, lastName, Sub_Categoria, campaign, medium, userId, singular_duplicate, test_duplicate, row_numbers, extCode, Comercio, RC, session_count_email, session_count_userId, eventTime_ac, gclid, country, state, town, zipcode, policy_start_date, policy_end_date, floors, goods, surface, inauguration_year, locationRisk, numLocals, shop_postcalcode, refurbishing_year, shop_region, electronic_protections, fire_protections, theft_protections, watchman_protections, Type, eventTime_vg, gross_income, money_invested, furnitureWithEquipment, merchandise, ownership, eventTime_off, monthlypayment1, premiumPrice1, monthlypayment2, premiumPrice2, monthlypayment3, premiumPrice3, provider, eventTime_fd, birthdate, documenttype, offerchosen, billingcycle, eventTime_pm, subcription_id, intent_status, endDate, paymenttype, policynumber, policy_url, policyprice, currentprice, Sales Date, Sub-categoria, Billing Cycle, Policy Number, Policy URL, Current Price, Policy Price, Start Date, Payment Type, Persona, Fecha Anulación, Razon anulación, Aseguradora, Online/offline, Renovado, Precio Renovacion, Tipo de póliza, Unnamed: 86, Match email, Match telefono, Semana, Mes, WP, Dia Lead, quote, sale, Categoria, time_diff_ay_ac, time_diff_ac_vg, time_diff_vg_ff, time_diff_ff_fd, time_diff_fd_pm, city, ...]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r\"C:\\Users\\Álvaro\\Final-project\\Data\\df_after_EDA.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 182)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.4 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-26d73d132359>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sale'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sale'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Training data shape X: {X_train.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2129\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2130\u001b[0m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[1;32m-> 2131\u001b[1;33m                                               default_test_size=0.25)\n\u001b[0m\u001b[0;32m   2132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   1812\u001b[0m             \u001b[1;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m             'aforementioned parameters.'.format(n_samples, test_size,\n\u001b[1;32m-> 1814\u001b[1;33m                                                 train_size)\n\u001b[0m\u001b[0;32m   1815\u001b[0m         )\n\u001b[0;32m   1816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.4 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "X = df.drop(columns='sale')\n",
    "y = df['sale']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.4)\n",
    "\n",
    "print(f'Training data shape X: {X_train.shape}')\n",
    "\n",
    "\n",
    "print(f'Testing data shape X: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(data=df, x='sale', palette={1: \"Gold\", 0: \"Lightblue\"})\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "plt.show()\n",
    "\n",
    "val_counts = round(df.sale.value_counts(normalize=True)*100, 2)\n",
    "print(f'The split between two classes (sale and not sale):\\n{val_counts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "estimator = RandomForestClassifier(criterion='entropy', \n",
    "                                   max_depth=2, \n",
    "                                   max_features='auto', \n",
    "                                   n_estimators=200, \n",
    "                                   random_state=42) \n",
    "\n",
    "estimator_name = 'RandomForestClassifier'\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()), ('clf', estimator)])\n",
    "model_results_tr_ts(pipeline, X_train, y_train, X_test, y_test, estimator_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances.nlargest(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importance.nñargest(25).reset_index().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(pipeline.steps[1][1].feature_importances_, \n",
    "                             index=df.drop(['sale'], axis=1).columns)\n",
    "\n",
    "feat_importances.nlargest(50).sort_values().plot(kind='barh', color='#1fb299')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(criterion='entropy', \n",
    "                               max_depth=2, \n",
    "                               max_features='auto', \n",
    "                               n_estimators=200, \n",
    "                               random_state=42) \n",
    "# Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Extract single tree\n",
    "estimator = model.estimators_[2]\n",
    "\n",
    "# Export as dot file\n",
    "dot_data = StringIO()\n",
    "export_graphviz(estimator, out_file=dot_data, \n",
    "                feature_names = df.drop(['sale'], axis=1).columns,\n",
    "                class_names = ['not_churned', 'churned'],\n",
    "                rounded=True, proportion=False, precision=2, filled=True)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit depth of tree to 3 levels\n",
    "rf_small = RandomForestRegressor(n_estimators=10, max_depth = 3)\n",
    "rf_small.fit(train_features, train_labels)\n",
    "# Extract the small tree\n",
    "tree_small = rf_small.estimators_[5]\n",
    "# Save the tree as a png image\n",
    "export_graphviz(tree_small, out_file = 'small_tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n",
    "(graph, ) = pydot.graph_from_dot_file('small_tree.dot')\n",
    "graph.write_png('small_tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The lists of the important features\n",
    "\n",
    "imp_features_to_filter_short = ['newsletter', 'master_channel_f', 'registration_payment', \n",
    "                                'with_promotion', 'sex', 'master_channel_a','master_channel_e', \n",
    "                                'master_channel_de', 'age', 'income_per_region', 'master_channel_s', \n",
    "                                'email_domain_ce', 'reg_year', 'churned_1_month']\n",
    "\n",
    "imp_features_to_filter_long = ['newsletter', 'master_channel_f', 'registration_payment', \n",
    "                               'with_promotion', 'sex', 'master_channel_a','master_channel_e', \n",
    "                               'master_channel_de', 'age', 'income_per_region', 'master_channel_s', \n",
    "                               'email_domain_ce', 'email_domain_oe', 'is_lead', 'population_per_region',\n",
    "                               'days_from_first_click', 'email_domain_ge', 'email_domain_ae', \n",
    "                               'box_satisfaction', 'reg_month_b', 'master_channel_ps', 'reg_month_c',\n",
    "                               'email_domain_hc', 'reg_month_k', 'reg_year', 'churned_1_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "X_test = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "          'ExtraTreesClassifier': ExtraTreesClassifier(random_state=42), \n",
    "          'LogisticRegression': LogisticRegression(random_state=42), \n",
    "          'SVC': SVC(random_state=42),\n",
    "          'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "          'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42),\n",
    "          'LinearSVC': LinearSVC(random_state=42),\n",
    "         }\n",
    "\n",
    "models_df_test = pd.concat([model_results_test_df(model, X_train, y_train, X_test, y_test, name) for (name, model) in models.items()])\n",
    "models_df_test.reset_index(inplace=True, drop=True)\n",
    "models_df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic Minority Oversampling Technique (SMOTE). It works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversamp_undersamp = SMOTETomek(sampling_strategy=0.6) #default sampling strategy\n",
    "X_train_b, y_train_b = oversamp_undersamp.fit_sample(X_train, y_train)\n",
    "X_train_b.shape, y_train_b.shape\n",
    "y_train_b.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "estimator = RandomForestClassifier(criterion='entropy', \n",
    "                                   max_depth=2, \n",
    "                                   max_features='auto', \n",
    "                                   n_estimators=200, \n",
    "                                   random_state=42) \n",
    "\n",
    "estimator_name = 'RandomForestClassifier'\n",
    "\n",
    "pipeline = Pipeline([('scaler', StandardScaler()), ('clf', estimator)])\n",
    "model_results_tr_ts(pipeline, X_train_b, y_train_b, X_test, y_test, estimator_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "          'ExtraTreesClassifier': ExtraTreesClassifier(random_state=42), \n",
    "          'LogisticRegression': LogisticRegression(random_state=42), \n",
    "          'SVC': SVC(random_state=42),\n",
    "          'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "          'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42),\n",
    "          'LinearSVC': LinearSVC(random_state=42),\n",
    "         }\n",
    "\n",
    "models_df_test = pd.concat([model_results_test_df(model, X_train_b, y_train_b, X_test, y_test, name) for (name, model) in models.items()])\n",
    "models_df_test.reset_index(inplace=True, drop=True)\n",
    "models_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "          'ExtraTreesClassifier': ExtraTreesClassifier(random_state=42),\n",
    "          'LogisticRegression': LogisticRegression(random_state=42), \n",
    "          'SVC': SVC(random_state=42),\n",
    "          'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "          'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42),\n",
    "          'LinearSVC': LinearSVC(random_state=42),\n",
    "         }\n",
    "\n",
    "# Defining values for the parameters\n",
    "random_grid = {'n_estimators': [100, 200, 300],\n",
    "              'max_features': ['auto', 'sqrt', 'log2'],\n",
    "              'max_depth' : [2, 5, 10],\n",
    "              'criterion' :['gini', 'entropy']}\n",
    "\n",
    "xgb_grid   = {'learning_rate': [0.01],\n",
    "              'n_estimators': [300, 700],\n",
    "              'max_depth':[7, 8],\n",
    "              'reg_alpha':[0.3, 0.4]}\n",
    "\n",
    "extra_grid = {'n_estimators': [20, 50, 100],\n",
    "             'max_features': ['auto', 'sqrt', 'log2'],\n",
    "             'min_samples_split': [2, 4, 6]}\n",
    "\n",
    "lgbm_grid  = {'num_leaves': [10, 30, 50], \n",
    "             'min_child_samples': [20, 40, 60], \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2],\n",
    "             'reg_alpha': [1e-1, 5],\n",
    "             'reg_lambda': [1e-1, 5]}\n",
    "\n",
    "c_values   = list(10.0**np.arange(-3, 3))\n",
    "log_grid   = [{'C': c_values,\n",
    "              'penalty': ['l2'],\n",
    "              'solver' : ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "              'multi_class': ['ovr', 'warn']},\n",
    "              {'C': c_values,\n",
    "              'penalty': ['l1'],\n",
    "              'solver' : ['liblinear', 'saga'],\n",
    "              'multi_class': ['ovr', 'warn']}]\n",
    "\n",
    "svc_grid   = {'C': [0.1, 1],\n",
    "              'gamma': [1, 0.1],\n",
    "              'kernel': ['rbf']}\n",
    "\n",
    "knn_grid   = {'n_neighbors': [5,7,9,11,13,15,17,19],\n",
    "              'weights': ['uniform', 'distance'],\n",
    "              'metric' : ['euclidean', 'manhattan']}\n",
    "\n",
    "depth      = list(np.arange(2, 10))\n",
    "dtree_grid = {'criterion': ['gini', 'entropy'],\n",
    "              'splitter': ['best', 'random'],\n",
    "              'max_depth' : depth}\n",
    "\n",
    "c_values   = list(10.0**np.arange(-3, 3))\n",
    "lsvc_grid  = [{'C': c_values,\n",
    "              'multi_class': ['ovr', 'crammer_singer'],\n",
    "              'penalty': ['l1'],\n",
    "              'loss': ['squared_hinge'],\n",
    "              'dual': [0]},\n",
    "              {'C': c_values,\n",
    "              'multi_class': ['ovr', 'crammer_singer'],\n",
    "              'penalty': ['l2'],\n",
    "              'loss': ['squared_hinge'],\n",
    "              'dual': [1]}]\n",
    "\n",
    "\n",
    "grids = [random_grid, extra_grid, log_grid, svc_grid, knn_grid, dtree_grid,lsvc_grid]\n",
    "\n",
    "# Defining cv\n",
    "strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "appended_dfs = []\n",
    "for (name, model), param_grid in zip (models.items(), grids):\n",
    "    # Runing grid search\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=strat_k_fold, n_jobs=-1, \n",
    "                        scoring='f1')\n",
    "    # Fitting the grid and use it to find the best estimator on train data\n",
    "    grid.fit(X_train_b, y_train_b) \n",
    "    # Creating the variable for best etimator \n",
    "    best_estimator = grid.best_estimator_\n",
    "    # Conducting model metrics on test data\n",
    "    models_df_test = model_results_hyp_test_cm_cr_df(best_estimator, X_test, y_test, name)\n",
    "    # Printing best parameters\n",
    "    print(f'Best parameters: {grid.best_params_}')\n",
    "    appended_dfs.append(models_df_test)\n",
    "\n",
    "model_results_hyp = pd.concat(appended_dfs, ignore_index = True)\n",
    "model_results_hyp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
